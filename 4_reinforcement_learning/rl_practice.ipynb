{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "# import gym\n",
    "import pygame\n",
    "from algorithms.rl import RL\n",
    "from examples.test_env import TestEnv\n",
    "from algorithms.planner import Planner\n",
    "import itertools\n",
    "\n",
    "# grid search\n",
    "from examples.grid_search import GridSearch\n",
    "from algorithms.planner import Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'phys2d/CartPole-v0', 'phys2d/CartPole-v1', 'phys2d/Pendulum-v0', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'tabular/Blackjack-v0', 'tabular/CliffWalking-v0', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'GymV21Environment-v0', 'GymV26Environment-v0'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.envs.registry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(500)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning_grid_search(env, epsilon_decay, iters):\n",
    "    for i in itertools.product(epsilon_decay, iters):\n",
    "        print(\"running q-learn -- with epsilon decay: \", i[0],  \" iterations: \", i[1])\n",
    "        Q, V, pi, Q_track, pi_track = RL(env).q_learning(epsilon_decay_ratio=i[0], n_episodes=i[1])\n",
    "\n",
    "def sarsa_grid_search(env, epsilon_decay, iters):\n",
    "    for i in itertools.product(epsilon_decay, iters):\n",
    "        print(\"running sarsa -- with epsilon decay: \", i[0],  \" iterations: \", i[1])\n",
    "        Q, V, pi, Q_track, pi_track = RL(env).sarsa(epsilon_decay_ratio=i[0], n_episodes=i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lboad\\Documents\\1_omscs\\machine_learning\\4_reinforcement_learning\\rl_practice.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lboad/Documents/1_omscs/machine_learning/4_reinforcement_learning/rl_practice.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m frozen_lake \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mMountainCar-v0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lboad/Documents/1_omscs/machine_learning/4_reinforcement_learning/rl_practice.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Q-learning\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lboad/Documents/1_omscs/machine_learning/4_reinforcement_learning/rl_practice.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m Q, V, pi, Q_track, pi_track \u001b[39m=\u001b[39m RL(frozen_lake\u001b[39m.\u001b[39;49menv)\u001b[39m.\u001b[39;49mq_learning()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lboad/Documents/1_omscs/machine_learning/4_reinforcement_learning/rl_practice.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m test_scores \u001b[39m=\u001b[39m TestEnv\u001b[39m.\u001b[39mtest_env(env\u001b[39m=\u001b[39mfrozen_lake\u001b[39m.\u001b[39menv, render\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, user_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, pi\u001b[39m=\u001b[39mpi)\n",
      "File \u001b[1;32mc:\\Users\\lboad\\py_venvs\\ml\\lib\\site-packages\\utils\\decorators.py:9\u001b[0m, in \u001b[0;36mprint_runtime.<locals>.wrapper_print_runtime\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper_print_runtime\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m      8\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 9\u001b[0m     value \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     10\u001b[0m     end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     11\u001b[0m     running_time \u001b[39m=\u001b[39m end \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\lboad\\py_venvs\\ml\\lib\\site-packages\\algorithms\\rl.py:145\u001b[0m, in \u001b[0;36mRL.q_learning\u001b[1;34m(self, nS, nA, convert_state_obs, gamma, init_alpha, min_alpha, alpha_decay_ratio, init_epsilon, min_epsilon, epsilon_decay_ratio, n_episodes)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[39m----------------------------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39m    Log of complete policy for each episode\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m nS \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     nS\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mn\n\u001b[0;32m    146\u001b[0m \u001b[39mif\u001b[39;00m nA \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     nA\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "frozen_lake = gym.make('FrozenLake8x8-v1', render_mode=None)\n",
    "# frozen_lake = gym.make('MountainCar-v0')\n",
    "\n",
    "# Q-learning\n",
    "Q, V, pi, Q_track, pi_track = RL(frozen_lake.env).q_learning()\n",
    "\n",
    "test_scores = TestEnv.test_env(env=frozen_lake.env, render=True, user_input=False, pi=pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_decay = [.4, .7, .9]\n",
    "iters = [500, 5000, 50000]\n",
    "GridSearch.Q_learning_grid_search(frozen_lake.env, epsilon_decay, iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot state values\n",
    "from examples.plots import Plots\n",
    "frozen_lake = gym.make('FrozenLake8x8-v1', render_mode=None)\n",
    "V, V_track, pi = Planner(frozen_lake.env.P).policy_iteration()\n",
    "Plots.grid_values_heat_map(V, \"State Values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import planner "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
